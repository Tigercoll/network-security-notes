---
title: Day032：日志与可观测性 - 可视化与告警策略
tags:
  - 网络
  - 安全
  - 学习计划
categories:
  - 网络安全
abbrlink: ac4cea53
date: 2026-01-25 00:00:00
updated: 2026-01-25 00:00:00

---
# Day032：日志与可观测性 - 可视化与告警策略

- 日期：2026-01-25
- 周次：第5周

## 学习目标

今天你将掌握日志可视化和告警的基础知识：

- **构建基础仪表盘**：使用 Kibana 创建监控仪表盘
- **设计简单告警规则**：配置基于阈值的告警
- **理解告警策略**：了解告警抑制、分组、降噪的方法
- **掌握告警通知**：能配置告警通知方式（邮件、Webhook）

---

<!--more-->

## 学习内容

### 1️⃣ 可视化基础

#### 1.1 关键指标选择

| 指标类别 | 具体指标 | 用途 |
|---------|---------|------|
| **流量指标** | QPS（每秒请求数）| 监控系统负载 |
| | TPS（每秒事务数）| 监控业务吞吐 |
| | 带宽使用量 | 监控网络消耗 |
| **延迟指标** | P50/P95/P99 | 监控响应时间分布 |
| | 平均响应时间 | 监控整体性能 |
| **错误指标** | 错误率 | 监控错误占比 |
| | HTTP 5xx 错误数 | 监控服务器端错误 |
| | HTTP 4xx 错误数 | 监控客户端错误 |
| **可用性指标** | 服务可用率 | 监控服务健康 |
| | 心跳成功/失败 | 监控存活状态 |
| **资源指标** | CPU 使用率 | 监控 CPU 负载 |
| | 内存使用率 | 监控内存消耗 |
| | 磁盘使用率 | 监控磁盘空间 |
| | 网络连接数 | 监控并发连接 |

---

#### 1.2 指标计算方法

**1. QPS（Queries Per Second）**

```json
// ES 查询
GET /logs-*/_search
{
  "size": 0,
  "query": {
    "range": {
      "timestamp": {
        "gte": "now-1m"
      }
    }
  }
}

// 计算：QPS = 总请求数 / 60 秒
```

**2. 错误率**

```json
// ES 查询
GET /logs-*/_search
{
  "size": 0,
  "aggs": {
    "total_requests": {
      "value_count": {
        "field": "_id"
      }
    },
    "error_requests": {
      "filter": {
        "range": {
          "http.status_code": {
            "gte": 400
          }
        }
      }
    }
  }
}

// 计算：错误率 = error_requests / total_requests * 100%
```

**3. P95/P99 响应时间**

```json
GET /logs-*/_search
{
  "size": 0,
  "query": {
    "exists": {
      "field": "http.duration_ms"
    }
  },
  "aggs": {
    "percentiles": {
      "percentiles": {
        "field": "http.duration_ms",
        "percents": [50, 95, 99]
      }
    }
  }
}
```

---

#### 1.3 可视化图表类型

| 图表类型 | 用途 | 数据类型 |
|---------|------|---------|
| **条形图** | 分类数据对比 | 离散类别（日志级别、服务）|
| **折线图** | 时间趋势 | 时间序列数据（QPS、错误率）|
| **面积图** | 累计趋势 | 时间序列（累计请求数）|
| **饼图** | 占比分析 | 分类占比（错误类型）|
| **仪表盘** | 单一指标 | 实时数据（CPU、内存）|
| **热力图** | 二维分布 | 时间 × 分类（错误时间 × 类型）|
| **数据表** | 详细数据 | 日志列表 |
| **地图** | 地理位置 | IP 地理分布 |

---

### 2️⃣ 告警基础

#### 2.1 告警三要素

```
告警三要素
├─ 1. 阈值
│  ├─ 固定阈值（>= 5xx 错误数 > 100）
│  ├─ 动态阈值（相比上周同期增长 50%）
│  └─ 异常检测（基于统计模型）
│
├─ 2. 时间窗口
│  ├─ 滑动窗口（最近 5 分钟）
│  ├─ 固定窗口（每分钟）
│  └─ 累计窗口（最近 1 小时）
│
└─ 3. 告警级别
   ├─ P0（严重）：服务中断、数据丢失
   ├─ P1（高）：功能不可用、性能严重下降
   ├─ P2（中）：性能下降、错误率上升
   └─ P3（低）：潜在风险、配置警告
```

---

#### 2.2 告警级别定义

| 级别 | 影响范围 | 响应时间 | 示例 |
|------|---------|---------|------|
| **P0 / Critical** | 全部用户 | 立即（5 分钟内）| 服务宕机、数据丢失 |
| **P1 / High** | 大部分用户 | 15 分钟内 | API 不可用、数据库连接失败 |
| **P2 / Medium** | 部分功能 | 1 小时内 | 响应时间变慢、错误率上升 |
| **P3 / Low** | 潜在风险 | 24 小时内 | 磁盘空间不足、证书即将过期 |

---

#### 2.3 告警策略

**1. 阈值告警**

```json
// 规则：错误数超过 100
{
  "condition": {
    "compare": "gt",
    "value": 100,
    "metric": "error_count",
    "window": "5m"
  }
}
```

**2. 百分比告警**

```json
// 规则：错误率超过 5%
{
  "condition": {
    "compare": "gt",
    "value": 0.05,
    "metric": "error_rate",
    "window": "5m"
  }
}
```

**3. 趋势告警**

```json
// 规则：错误率相比上周同期增长 50%
{
  "condition": {
    "compare": "gt",
    "value": 0.5,
    "metric": "error_rate_growth",
    "baseline": "last_week_same_time"
  }
}
```

**4. 组合告警**

```json
// 规则：错误数 > 100 且 持续 5 分钟
{
  "conditions": [
    {
      "metric": "error_count",
      "compare": "gt",
      "value": 100
    },
    {
      "metric": "duration",
      "compare": "gt",
      "value": "5m"
    }
  ],
  "operator": "and"
}
```

---

### 3️⃣ 告警抑制与降噪

#### 3.1 告警抑制策略

| 策略 | 说明 | 适用场景 |
|------|------|---------|
| **抑制窗口** | 在窗口期内只触发一次 | 避免重复告警 |
| **依赖抑制** | 依赖其他告警 | 告警关联性 |
| **时间抑制** | 指定时间段不触发 | 维护窗口 |
| **频率限制** | 限制告警频率 | 防止告警风暴 |

**示例：抑制窗口**

```
规则：5xx 错误数 > 100
抑制窗口：10 分钟

触发：
- 10:00: 检测到 120 个错误 → 触发告警
- 10:05: 检测到 150 个错误 → 抑制（在窗口内）
- 10:10: 检测到 130 个错误 → 触发告警（窗口已过期）
```

---

#### 3.2 告警分组

**目的**：将相关告警分组，减少通知次数。

**分组方式**：

| 分组方式 | 示例 |
|---------|------|
| **按主机分组** | 同一主机的多个告警合并 |
| **按服务分组** | 同一服务的多个告警合并 |
| **按严重级别分组** | 同一级别的告警合并 |
| **按根因分组** | 相同根因的告警合并 |

**示例**：

```
原始告警：
1. web-01: CPU 使用率 > 80%
2. web-01: 内存使用率 > 80%
3. web-01: 响应时间 P95 > 1000ms
4. web-02: CPU 使用率 > 80%

分组后告警：
[主机分组] web-01:
  - CPU 使用率 > 80%
  - 内存使用率 > 80%
  - 响应时间 P95 > 1000ms

[主机分组] web-02:
  - CPU 使用率 > 80%
```

---

#### 3.3 降噪策略

| 降噪方法 | 说明 | 示例 |
|---------|------|------|
| **白名单** | 指定条件不触发告警 | 健康检查、测试环境 |
| **聚合规则** | 相似告警合并 | 同类错误合并 |
| **告警延迟** | 延迟触发告警 | 避免瞬时抖动 |
| **智能降噪** | 基于历史数据调整 | 周末夜间不告警 |

---

### 4️⃣ 告警通知

#### 4.1 通知渠道

| 渠道 | 特点 | 适用场景 |
|------|------|---------|
| **邮件** | 正式、可追溯 | P0/P1 级别告警 |
| **短信** | 即时、醒目 | P0 级别告警 |
| **IM（钉钉、飞书、Slack）** | 团队协作 | P1/P2 级别告警 |
| **电话** | 最紧急 | P0 级别告警 |
| **Webhook** | 灵活集成 | 自定义系统集成 |

---

#### 4.2 通知策略

**分级通知**：

| 级别 | 邮件 | 短信 | IM | 电话 |
|------|------|------|---|------|
| P0 | ✅ | ✅ | ✅ | ✅ |
| P1 | ✅ | ✅ | ✅ | - |
| P2 | ✅ | - | ✅ | - |
| P3 | ✅ | - | - | - |

**升级策略**：

```
规则：P1 告警 15 分钟未确认 → 升级为 P0
      P1 告警 30 分钟未确认 → 电话通知

执行：
1. 10:00: 触发 P1 告警 → 发送邮件 + IM
2. 10:15: 未确认 → 发送短信
3. 10:30: 未确认 → 电话通知
```

---

### 5️⃣ 仪表盘设计

#### 5.1 仪表盘布局

```
┌─────────────────────────────────────────────────────────┐
│                    服务监控仪表盘                        │
├─────────────────────────────────────────────────────────┤
│                                                           │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐      │
│  │  QPS       │  │ 错误率     │  │ P95 延迟   │      │
│  │  [折线图]  │  │  [折线图]  │  │  [折线图]  │      │
│  └────────────┘  └────────────┘  └────────────┘      │
│                                                           │
│  ┌─────────────────────────────────────┐                │
│  │  错误数量趋势（按日志级别）        │                │
│  │  [堆叠面积图]                   │                │
│  └─────────────────────────────────────┘                │
│                                                           │
│  ┌─────────────────────────────────────┐                │
│  │  Top 10 错误日志                  │                │
│  │  [数据表]                         │                │
│  └─────────────────────────────────────┘                │
│                                                           │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐      │
│  │  CPU 使用率 │  │ 内存使用率 │  │ 磁盘使用率 │      │
│  │  [仪表盘]  │  │  [仪表盘]  │  │  [仪表盘]  │      │
│  └────────────┘  └────────────┘  └────────────┘      │
└─────────────────────────────────────────────────────────┘
```

---

#### 5.2 仪表盘组件清单

| 位置 | 组件 | 数据源 | 图表类型 |
|------|------|--------|---------|
| 顶部左侧 | QPS | logs-* | 折线图（Date Histogram + Count）|
| 顶部中间 | 错误率 | logs-* | 折线图（Filter + Count）|
| 顶部右侧 | P95 延迟 | logs-* | 折线图（Percentiles Aggregation）|
| 中部 | 错误趋势 | logs-* | 堆叠面积图（Date Histogram + Terms Aggregation）|
| 底部左侧 | Top 10 错误 | logs-* | 数据表（Terms Aggregation + Top Hits）|
| 底部中间 | CPU | 系统指标 | 仪表盘（Stats Aggregation）|
| 底部右侧 | 内存 | 系统指标 | 仪表盘（Stats Aggregation）|

---

### 6️⃣ Kibana Alerting 配置

#### 6.1 创建告警规则

**步骤**：

1. 进入 Kibana：Alerting → Rules
2. 点击 Create rule
3. 选择数据源：Elasticsearch queries
4. 定义查询条件

**示例：5xx 错误告警**

```
Rule name: HTTP 5xx 错误告警
Check interval: 1 minute

Query:
GET /logs-*/
{
  "query": {
    "bool": {
      "filter": [
        { "range": {
          "http.status_code": {
            "gte": 500
          }
        }},
        { "range": {
          "timestamp": {
            "gte": "now-1m"
          }
        }}
      ]
    }
  }
}

Condition:
  - if: count > 100
    then: trigger alert

Actions:
  - Send email to: admin@example.com
  - Send Slack webhook
```

---

#### 6.2 创建告警动作

**邮件动作**：

```json
{
  "actionTypeId": ".email",
  "params": {
    "to": "admin@example.com",
    "subject": "Alert: HTTP 5xx 错误数过高",
    "body": "检测到过去 1 分钟内 {{context.payload.hits.total}} 个 5xx 错误"
  }
}
```

**Slack Webhook 动作**：

```json
{
  "actionTypeId": ".webhook",
  "params": {
    "url": "https://hooks.slack.com/services/YOUR/WEBHOOK",
    "method": "POST",
    "headers": {
      "Content-Type": "application/json"
    },
    "body": "{\"text\": \"Alert: HTTP 5xx 错误数过高 - {{context.payload.hits.total}} 个错误\"}"
  }
}
```

---

#### 6.3 告警连接器

**连接器类型**：

| 连接器 | 功能 |
|-------|------|
| **Email connector** | 发送邮件 |
| **Slack connector** | 发送 Slack 消息 |
| **Webhook connector** | 发送自定义 HTTP 请求 |
| **Index connector** | 将告警写入索引 |
| **Jira connector** | 创建 Jira 工单 |
| **ServiceNow connector** | 创建 ServiceNow 工单 |

---

### 7️⃣ 告警最佳实践

#### 7.1 告警设计原则

| 原则 | 说明 |
|------|------|
| **可操作性** | 告警信息应包含处理建议 |
| **准确性** | 减少误报和漏报 |
| **及时性** | 告警应在问题发生后尽快触发 |
| **完整性** | 告警应包含足够的上下文信息 |
| **避免疲劳** | 合理设置抑制窗口，避免告警风暴 |

---

#### 7.2 告警消息模板

**优秀告警消息**：

```
【P0/严重】服务宕机告警 - web-01

时间：2026-01-23 10:30:00
主机：web-01 (192.168.1.10)
服务：nginx
影响：全部用户无法访问

【可能原因】：
1. 服务进程崩溃
2. 系统资源耗尽
3. 网络连接中断

【排查步骤】：
1. SSH 登录 web-01
2. 检查服务状态：systemctl status nginx
3. 检查系统资源：top, df -h
4. 查看日志：tail -f /var/log/nginx/error.log

【通知人】：
- 运维团队：ops-team@example.com
- 开发团队：dev-team@example.com
```

---

## 实践任务（合法授权范围内）

> **注意**：请在你自己的测试环境中执行以下任务。

---

### 任务 1（必做）：在 Kibana 中创建可视化图表

**目标**：创建 3 个不同类型的可视化图表。

**步骤**：

1. **创建条形图（按日志级别统计）**

- 进入 Kibana：http://localhost:5601
- 进入 Visualize Library
- 点击 Create Visualization → Bar Chart
- 选择索引模式：`logs-*`
- Buckets → Split aggregation
  - Aggregation: Terms
  - Field: level.keyword
  - Order by: Count
  - Direction: Descending
  - Size: 10
- Metrics → Set Count
- 点击 Update
- 保存为：`Logs by Level`

2. **创建折线图（按时间统计 QPS）**

- 进入 Visualize Library
- 点击 Create Visualization → Line Chart
- 选择索引模式：`logs-*`
- X-Axis → Date Histogram
  - Field: timestamp
  - Minimum Interval: minute
- Y-Axis → Count
- 点击 Update
- 保存为：`QPS over Time`

3. **创建仪表盘（CPU 监控）**

- 进入 Visualize Library
- 点击 Create Visualization → Gauge
- 选择索引模式：`metrics-*`（假设有系统指标）
- Metrics → Max
  - Field: system.cpu.usage
  - 值设置：显示最大值
- Options → Gauge ranges
  - 设置红/黄/绿区间
- 点击 Update
- 保存为：`CPU Usage`

---

### 任务 2（必做）：创建监控仪表盘

**目标**：创建一个包含多个图表的监控仪表盘。

**步骤**：

1. **进入 Dashboard**

- 点击 Dashboard → Create Dashboard
- 选择 Blank Dashboard

2. **添加可视化**

- 点击 Add from library
- 依次添加以下图表：
  - `Logs by Level`（条形图）
  - `QPS over Time`（折线图）
  - `CPU Usage`（仪表盘）
  - `Memory Usage`（仪表盘）
  - `Disk Usage`（仪表盘）

3. **调整布局**

- 拖拽调整图表位置
- 设置图表大小
- 添加标题和描述

4. **保存仪表盘**

- 点击 Save
- 名称：`Monitoring Dashboard`
- 存储时间：`Last 7 days`

---

### 任务 3（必做）：创建告警规则

**目标**：在 Kibana Alerting 中创建 2 条告警规则。

**步骤**：

1. **创建 HTTP 5xx 错误告警**

- 进入 Alerting → Rules
- 点击 Create rule
- 选择类型：Elasticsearch query
- Rule name：`HTTP 5xx Error Alert`
- Index pattern：`logs-*`
- 查询条件：

```json
{
  "query": {
    "bool": {
      "filter": [
        { "term": { "http.status_code": { "value": 500 } } },
        { "term": { "http.status_code": { "value": 501 } } },
        { "term": { "http.status_code": { "value": 502 } } },
        { "term": { "http.status_code": { "value": 503 } } },
        { "term": { "http.status_code": { "value": 504 } } }
      ]
    }
  }
}
```

- 聚合：Count
- 阈值：`greater than 10`
- 检查间隔：`Every 1 minute`
- 时间窗口：`Last 5 minutes`

- 动作：Add action
  - 选择：Email
  - 发送至：`admin@example.com`
  - Subject：`Alert: High 5xx Error Rate`

2. **创建服务不可用告警**

- Rule name：`Service Down Alert`
- Index pattern：`logs-*`
- 查询条件：

```json
{
  "query": {
    "bool": {
      "must_not": [
        { "exists": { "field": "message" } }
      ]
    }
  }
}
```

- 聚合：Count
- 阈值：`less than 1`
- 检查间隔：`Every 1 minute`
- 时间窗口：`Last 2 minutes`

---

### 任务 4（必做）：配置告警通知

**目标**：配置告警通过邮件和 Webhook 通知。

**步骤**：

1. **配置邮件通知**

- 进入 Alerting → Connectors
- 点击 Create connector
- 选择：Email
- 配置：
  - Host: `smtp.example.com`
  - Port: `587`
  - Username: `alert@example.com`
  - Password: `******`
  - From: `noreply@example.com`
- 保存为：`Email Connector`

2. **配置 Slack Webhook**

- 进入 Alerting → Connectors
- 点击 Create connector
- 选择：Webhook
- 配置：
  - Name：`Slack Connector`
  - URL：`https://hooks.slack.com/services/YOUR/WEBHOOK`
  - Method：`POST`
  - Headers：`Content-Type: application/json`
  - Body：`{"text": "{{context.message}}"}`
- 保存为：`Slack Connector`

3. **将通知连接到告警规则**

- 编辑告警规则
- 在 Actions 中选择连接器
- 配置消息模板

---

### 任务 5（进阶）：配置告警抑制

**目标**：配置告警抑制窗口，避免重复告警。

**步骤**：

1. **编辑告警规则**

- 进入 Alerting → Rules
- 选择 `HTTP 5xx Error Alert`
- 点击 Edit

2. **配置抑制规则**

- 在 Rule settings 中找到：
  - Suppress for: `10 minutes`
  - Suppress on missing data: `No`

3. **保存规则**

---

### 任务 6（进阶）：配置告警分组

**目标**：配置告警分组，将相关告警合并。

**步骤**：

1. **创建告警规则分组**

- 进入 Alerting → Rules
- 点击 Create rule group
- 名称：`HTTP Error Alert Group`
- 添加 2 个规则：
  - `HTTP 5xx Error Alert`
  - `HTTP 4xx Error Alert`

2. **配置分组策略**

- 在 Group settings 中配置：
  - Group by: `hostname`
  - Limit: `10`

3. **保存分组**

---

### 任务 7（进阶）：配置告警升级

**目标**：配置 P1 告警 15 分钟未处理升级为 P0。

**步骤**：

1. **创建升级规则**

- 进入 Alerting → Rules
- 创建新规则：`Alert Escalation Rule`
- 类型：Elasticsearch query
- 查询条件：查找 P1 告警

```json
{
  "query": {
    "term": {
      "alert.level": "P1"
    }
  }
}
```

- 阈值：`greater than 0`
- 检查间隔：`Every 5 minutes`

2. **配置升级逻辑**

- 在 Scripting 或 Action 中配置：
  - 查询告警最后更新时间
  - 如果超过 15 分钟未处理，升级为 P0

3. **配置通知**

- 升级后发送 P0 级别通知
- 包括邮件、短信、电话

---

## 巩固练习（题与复盘）

---

### 练习 1：如何减少告警噪声？

**思路提示**：

- 抑制窗口
- 告警分组
- 白名单
- 智能降噪

---

### 练习 2：编写告警分级策略

**任务**：为以下场景设计告警分级和响应时间。

**场景**：

1. HTTP 5xx 错误率超过 10%
2. 服务响应时间 P95 > 2s
3. 磁盘使用率 > 80%
4. 数据库连接失败
5. 支付服务不可用

**要求**：

1. 定义告警级别（P0/P1/P2/P3）
2. 定义响应时间
3. 定义通知渠道
4. 定义升级策略

---

### 练习 3：设计仪表盘布局

**任务**：为生产环境设计一个完整的监控仪表盘。

**要求**：

1. 顶部：关键指标概览（QPS、错误率、P95 延迟）
2. 中部：服务状态（HTTP 服务、数据库、缓存）
3. 底部：资源使用（CPU、内存、磁盘、网络）
4. 右侧：Top 10 错误日志

---

### 练习 4：编写告警消息模板

**任务**：为 P0、P1、P2、P3 级别编写告警消息模板。

**要求**：

1. 包含告警级别
2. 包含问题描述
3. 包含影响范围
4. 包含可能原因
5. 包含排查步骤
6. 包含联系人

---

## 评估标准（达成判定）

- ✅ 创建了 3 个不同类型的可视化图表
- ✅ 创建了包含多个图表的监控仪表盘
- ✅ 创建了 2 条告警规则
- ✅ 配置了告警通知（邮件/Webhook）
- ✅ 配置了告警抑制
- ✅ 理解了告警分级和响应策略

---

## 学习成果达成情况（由学习者填写）
- 截图与证据：
- 关键命令与输出：
- 结论与反思：


## 集中参考答案（含思路）

### 题 1 参考答案

（待补充）


## 学习成果示例填写（可照抄）

> 可将"示例"内容替换为你自己的时间与截图文件名。

### 截图与证据（示例）

- 任务 1：`images/dayXXX_task1.png`

### 关键命令与输出（示例）

```
命令示例：
输出示例：
```

### 结论与反思（示例）

**我今天搞清楚了**：
- （示例）理解了核心概念

**我差点搞混的是**：
- （示例）某个易混淆点

**明天我要继续补的是**：
- （示例）下一步深入方向

**本次学习耗时**：约 2 小时

**掌握程度自评**：
- [x] 😃 完成了所有任务并理解原理
